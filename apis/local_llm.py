from typing import Tuple, Optional, Dict, Any
from termcolor import colored
import os

from .base_provider import BaseProvider
from .response_parser import ResponseParser

class LocalLLMProvider(BaseProvider):
    """Provider for local LLM using llama.cpp"""
    
    def __init__(self):
        self.llm = None
        self.model_path = None
        
    @property
    def name(self) -> str:
        return "local"
    
    @property
    def description(self) -> str:
        return f"Local LLM ({os.path.basename(self.model_path)})" if self.model_path else "Local LLM"
    
    def initialize(self, config: Dict[str, Any]) -> bool:
        """Initialize local LLM with configuration"""
        try:
            from llama_cpp import Llama
            
            model_path = config.get("path")
            n_ctx = config.get("n_ctx") or 2048
            n_threads = config.get("n_threads") or 4
            
            if not model_path:
                print(colored("⚠️ Model path not configured", "yellow"))
                return False
                
            if not os.path.exists(model_path):
                print(colored(f"⚠️ Model file not found: {model_path}", "yellow"))
                return False
                
            self.llm = Llama(
                model_path=model_path,
                n_ctx=n_ctx,
                n_threads=n_threads,
                verbose=False
            )
            
            self.model_path = model_path
            print(colored(f"✓ Local LLM initialized successfully with {os.path.basename(model_path)}", "green"))
            return True
            
        except Exception as e:
            print(colored(f"⚠️ Local LLM initialization failed: {str(e)}", "yellow"))
            return False
    
    def generate_command(self, user_input: str, os_type: str) -> Tuple[Optional[str], Optional[str]]:
        """Generate command using local LLM"""
        if not self.llm:
            return None, None
            
        try:
            prompt = f"""Convert this to a {os_type} terminal command.
            Only output the command, nothing else.
            Request: {user_input}
            Command:"""
            
            response = self.llm(
                prompt,
                max_tokens=50,
                temperature=0.7,
                stop=["\n"],
                echo=False
            )
            
            raw_command = response['choices'][0]['text'].strip()
            command = ResponseParser.clean_command(raw_command)
            
            if command:
                return command, f"Generated by local LLM"
            return None, None
            
        except Exception as e:
            print(colored(f"⚠️ Local LLM error: {str(e)}", "red"))
            return None, None
